{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Classification by Alol Prakash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: Perform supervised binary sentiment classification on labelled financial news headlines, where a news headline is either positive or negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data pre-processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import contractions\n",
    "import nlpaug.augmenter.word as naw\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#Machine Learning pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report, balanced_accuracy_score,log_loss\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "\n",
    "#Deep Learning pipeline\n",
    "from transformers import RobertaTokenizerFast, RobertaForSequenceClassification, Trainer, TrainingArguments,EarlyStoppingCallback\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>The international electronic industry company Elcoteq has laid off tens of employees from its Tallinn facility ; contrary to earlier layoffs the company contracted the ranks of its office workers , the daily Postimees reported .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>With the new production plant the company would increase its capacity to meet the expected increase in demand and would improve the use of raw materials and therefore increase the production profitability .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>According to the company 's updated strategy for the years 2009-2012 , Basware targets a long-term net sales growth in the range of 20 % -40 % with an operating profit margin of 10 % -20 % of net sales .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>FINANCING OF ASPOCOMP 'S GROWTH Aspocomp is aggressively pursuing its growth strategy by increasingly focusing on technologically more demanding HDI printed circuit boards PCBs .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962</th>\n",
       "      <td>negative</td>\n",
       "      <td>HELSINKI Thomson Financial - Shares in Cargotec fell sharply in early afternoon trade after the cargo handling group posted a surprise drop in April-June profits , which overshadowed the large number of new orders received during the three months .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963</th>\n",
       "      <td>negative</td>\n",
       "      <td>LONDON MarketWatch -- Share prices ended lower in London Monday as a rebound in bank stocks failed to offset broader weakness for the FTSE 100 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1964</th>\n",
       "      <td>negative</td>\n",
       "      <td>Operating profit fell to EUR 35.4 mn from EUR 68.8 mn in 2007 , including vessel sales gain of EUR 12.3 mn .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1965</th>\n",
       "      <td>negative</td>\n",
       "      <td>Net sales of the Paper segment decreased to EUR 221.6 mn in the second quarter of 2009 from EUR 241.1 mn in the second quarter of 2008 , while operating profit excluding non-recurring items rose to EUR 8.0 mn from EUR 7.6 mn .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1966</th>\n",
       "      <td>negative</td>\n",
       "      <td>Sales in Finland decreased by 10.5 % in January , while sales outside Finland dropped by 17 % .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1967 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         class  \\\n",
       "0     negative   \n",
       "1     positive   \n",
       "2     positive   \n",
       "3     positive   \n",
       "4     positive   \n",
       "...        ...   \n",
       "1962  negative   \n",
       "1963  negative   \n",
       "1964  negative   \n",
       "1965  negative   \n",
       "1966  negative   \n",
       "\n",
       "                                                                                                                                                                                                                                                          text  \n",
       "0                         The international electronic industry company Elcoteq has laid off tens of employees from its Tallinn facility ; contrary to earlier layoffs the company contracted the ranks of its office workers , the daily Postimees reported .  \n",
       "1                                               With the new production plant the company would increase its capacity to meet the expected increase in demand and would improve the use of raw materials and therefore increase the production profitability .  \n",
       "2                                                  According to the company 's updated strategy for the years 2009-2012 , Basware targets a long-term net sales growth in the range of 20 % -40 % with an operating profit margin of 10 % -20 % of net sales .  \n",
       "3                                                                           FINANCING OF ASPOCOMP 'S GROWTH Aspocomp is aggressively pursuing its growth strategy by increasingly focusing on technologically more demanding HDI printed circuit boards PCBs .  \n",
       "4                                                            For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .  \n",
       "...                                                                                                                                                                                                                                                        ...  \n",
       "1962  HELSINKI Thomson Financial - Shares in Cargotec fell sharply in early afternoon trade after the cargo handling group posted a surprise drop in April-June profits , which overshadowed the large number of new orders received during the three months .  \n",
       "1963                                                                                                          LONDON MarketWatch -- Share prices ended lower in London Monday as a rebound in bank stocks failed to offset broader weakness for the FTSE 100 .  \n",
       "1964                                                                                                                                              Operating profit fell to EUR 35.4 mn from EUR 68.8 mn in 2007 , including vessel sales gain of EUR 12.3 mn .  \n",
       "1965                        Net sales of the Paper segment decreased to EUR 221.6 mn in the second quarter of 2009 from EUR 241.1 mn in the second quarter of 2008 , while operating profit excluding non-recurring items rose to EUR 8.0 mn from EUR 7.6 mn .  \n",
       "1966                                                                                                                                                           Sales in Finland decreased by 10.5 % in January , while sales outside Finland dropped by 17 % .  \n",
       "\n",
       "[1967 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#resizing output display\n",
    "pd.set_option('max_colwidth', None)\n",
    "\n",
    "def import_clean_df(path:str):\n",
    "    ''' \n",
    "    Takes a path to the original data, reads data as pandas (pd) dataframe (df) and does intial cleaning:\n",
    "    (names columns, drops duplicate and blank rows, resets index) and returns the df\n",
    "\n",
    "    Args:\n",
    "        path(str): path to raw data\n",
    "    Returns:\n",
    "        dataframe(df): cleaned pd df\n",
    "    ''' \n",
    "    #reading in data and naming columns to class and text\n",
    "    raw = pd.read_csv(path,skipinitialspace=True, skip_blank_lines=True,encoding = \"ISO-8859-1\", names=[\"class\",\"text\"]) \n",
    "    #subset data to only positve and negative classes\n",
    "    raw_pn = raw[raw[\"class\"].isin([\"positive\",\"negative\"])]\n",
    "    #drop blank rows\n",
    "    raw_pn = raw_pn.dropna()\n",
    "    #drop duplicate rows\n",
    "    raw_pn = raw_pn.drop_duplicates()\n",
    "    #reset index\n",
    "    raw_pn = raw_pn.reset_index(drop=True)\n",
    "    return raw_pn\n",
    "\n",
    "#path to data\n",
    "filepath = \"archive/all-data.csv\"\n",
    "\n",
    "data = import_clean_df(filepath)\n",
    "#preview of data\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Frequency:\n",
      " positive    1363\n",
      "negative     604\n",
      "Name: class, dtype: int64\n",
      "\n",
      "Average word count per class:\n",
      " class\n",
      "negative    23.917219\n",
      "positive    24.692590\n",
      "Name: word_count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#class frequency\n",
    "print(\"Class Frequency:\\n\",data[\"class\"].value_counts())\n",
    "#word count per class\n",
    "data[\"word_count\"] = data[\"text\"].map(lambda x: len(x.split()))\n",
    "print(\"\\nAverage word count per class:\\n\",data.groupby(\"class\")[\"word_count\"].mean())\n",
    "#delete created col after use\n",
    "del data[\"word_count\"]"
   ]
  },
  
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data is split to avoid data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(df:pd.DataFrame):\n",
    "    ''' \n",
    "    This function takes a df, applies stratified k fold which proportionally splits the data to train and test data\n",
    "\n",
    "    Args:\n",
    "        dataframe(df): pd dataframe\n",
    "    Returns:\n",
    "        train_data(df): train dataframe\n",
    "        test_data(df): test dataframe\n",
    "    '''\n",
    "\n",
    "    #stratified k fold\n",
    "    skf = StratifiedKFold(n_splits = 5, \n",
    "                          random_state = 11,\n",
    "                          shuffle = True\n",
    "                          )\n",
    "    X = df[\"text\"] # collection of text\n",
    "    y = df[\"class\"] # class we want to predict\n",
    "\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    #combine data  \n",
    "    train_data = pd.concat([X_train,y_train], axis = 1)\n",
    "    train_data = train_data.reset_index(drop=True)\n",
    "\n",
    "    test_data = pd.concat([X_test,y_test], axis=1)\n",
    "    test_data = test_data.reset_index(drop=True)\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "#apply data split function\n",
    "train_data, test_data = split_train_test(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Frequency in train data:\n",
      " positive    1090\n",
      "negative     484\n",
      "Name: class, dtype: int64\n",
      "\n",
      "Class Frequency in test data:\n",
      " positive    273\n",
      "negative    120\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Class Frequency in train data:\\n\",train_data[\"class\"].value_counts())\n",
    "print(\"\\nClass Frequency in test data:\\n\",test_data[\"class\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cleaning and normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(df:pd.DataFrame):\n",
    "    ''' \n",
    "    This function takes a dataframe, takes the text column to removes extra whitespaces,blank rows and resets index,changes all text to lower case, \n",
    "    uses encode function to remove non ASCII characters, uses contractions library to expand common English contractions, removes stop words and \n",
    "    converts to lemmatized text.\n",
    "\n",
    "    Args:\n",
    "        dataframe(df): a data frame\n",
    "    Returns:\n",
    "        dataframe(df): cleaned and normalised dataframe\n",
    "    '''\n",
    "    #remove leading and trailing whitespace\n",
    "    df = df.apply(lambda x: x.str.strip())\n",
    "    #convert text to lowercase\n",
    "    df['text'] = df['text'].apply(lambda x: x.lower())\n",
    "    #remove non ASCII characters\n",
    "    df['text'] = df['text'].apply(lambda x: x.encode(\"ascii\",errors=\"ignore\").decode())\n",
    "    #expand common contractions\n",
    "    df['text'] = df['text'].apply(lambda x: [contractions.fix(word) for word in x.split()]).apply(lambda x:\" \".join(word for word in x))\n",
    "    #remove any punctuations not included in the below\n",
    "    df['text'] = df['text'].apply(lambda x: re.sub(r'[^\\w\\d\\s\\!\\?\\-\\%\\.]+', '', x, flags=re.S))\n",
    "    #any whitespace occuring more than once\n",
    "    df['text'] = df['text'].apply(lambda x: re.sub(r'\\s+', ' ', x, flags=re.M|re.S))\n",
    "    #any whitespace occuring more than once before the punctuation below\n",
    "    df['text'] = df['text'].apply(lambda x: re.sub(r'\\s+([.%!?-])', r'\\1', x, flags=re.M|re.S))\n",
    "    #any whitespace occuring more than once\n",
    "    df['text'] = df['text'].apply(lambda x: re.sub(r'\\s+', ' ', x, flags=re.M|re.S))\n",
    "    #remove leading and trailing whitespace\n",
    "    df = df.apply(lambda x: x.str.strip())\n",
    "    #remove blanks and reset the index\n",
    "    df = df.dropna()\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "#chose to lemmatize and remove stop words after augmentation as the augmented text were too simialr to original when lemmatising before augmentation\n",
    "\n",
    "#apply clean function to train and test data\n",
    "train_c,test_c = clean(train_data),clean(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data augmentation to train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using data augmentation to upsample the negative class (minority) to the size of the positive class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "606\n"
     ]
    }
   ],
   "source": [
    "#size of majority class\n",
    "maj_class_size = train_c[\"class\"].value_counts().max()\n",
    "#size of minority class\n",
    "min_class_size = train_c[\"class\"].value_counts().min()\n",
    "#the difference between class sizes\n",
    "diff_class_size = maj_class_size - min_class_size\n",
    "print(diff_class_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using synonym replacement via contextual word embeddings (CITE)\n",
    "aug = naw.ContextualWordEmbsAug(model_path='bert-base-uncased', action=\"substitute\")\n",
    "\n",
    "#separate the minority and majority rows into 2 dfs\n",
    "min_class_df = train_c[train_c['class'] == \"negative\"]\n",
    "maj_class_df = train_c[train_c['class'] == \"positive\"]\n",
    "\n",
    "#initialise the augmented text list\n",
    "aug_texts = []\n",
    "#for each row up to the size of the difference, augment the negative class and cycle through it till it augments up to the difference\n",
    "# and append to the augmented text list\n",
    "for i in range(diff_class_size):\n",
    "    augmented_text = aug.augment(min_class_df.iloc[i % min_class_size]['text'])\n",
    "    aug_texts.append(augmented_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create df from augmented data\n",
    "augmented_train = pd.DataFrame({'text': aug_texts, 'class': 'negative'})\n",
    "#convert list to str\n",
    "augmented_train['text'] = augmented_train['text'].astype(str)\n",
    "#remove ['']\n",
    "augmented_train['text'] = augmented_train['text'].str.strip(\"[]''\")\n",
    "#concatenate original cleaned negative df with augmented cleaned negative df\n",
    "combined_train = pd.concat([min_class_df, augmented_train])\n",
    "#concatenate this modified negative df with positive df\n",
    "train_caug = pd.concat([combined_train, maj_class_df])\n",
    "train_caug = train_caug.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cleaning round 2 - lemmatisation stop word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##load spacy library\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "#python -m spacy download en_core_web_sm #download the model\n",
    "\n",
    "def lemmatise_remove_stopwords(df:pd.DataFrame):\n",
    "    \"\"\"\n",
    "    This function lemmatises, removes stop words using spacy and does some extra regex replacements\n",
    "\n",
    "    Args:\n",
    "        dataframe(df): a data frame\n",
    "    Returns:\n",
    "        dataframe(df): lemmatised and cleaned dataframe\n",
    "    \"\"\"\n",
    "    #lemmatize text and remove stop words using spacy \n",
    "    df['text'] = df['text'].apply(lambda x: \" \".join(token.lemma_ for token in nlp(x) if not token.is_stop))\n",
    "    #any whitespace occuring more than once before the punctuation below\n",
    "    df['text'] = df['text'].apply(lambda x: re.sub(r'\\s+([.%!?-])', r'\\1', x, flags=re.M|re.S))\n",
    "    #any whitespace occuring more than once\n",
    "    df['text'] = df['text'].apply(lambda x: re.sub(r'\\s+', ' ', x, flags=re.M|re.S))\n",
    "    #remove leading and trailing whitespace\n",
    "    df = df.apply(lambda x: x.str.strip())\n",
    "    #remove blanks and reset the index\n",
    "    df = df.dropna()\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "#apply function to lemmatise,remove stopwords and clean\n",
    "train_caugl,test_cl = lemmatise_remove_stopwords(train_caug),lemmatise_remove_stopwords(test_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Before data augmentation:\n",
      " positive    1090\n",
      "negative     484\n",
      "Name: class, dtype: int64\n",
      "\n",
      "After data augmentation:\n",
      " negative    1090\n",
      "positive    1090\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#example augmentation\n",
    "#print(train_caugl.iloc[2].to_string())\n",
    "#print(train_caugl.iloc[486].to_string())\n",
    "\n",
    "print('\\nBefore data augmentation:\\n',train_c[\"class\"].value_counts())\n",
    "print('\\nAfter data augmentation:\\n',train_caugl['class'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(train: pd.DataFrame, test: pd.DataFrame):\n",
    "    ''' \n",
    "    This function takes 2 dataframes and encodes the classes in the train data and applies that to all dfs (train, test)\n",
    "\n",
    "    Args:\n",
    "        train(df): train dataframe\n",
    "        test(df): validation dataframe\n",
    "    Returns:\n",
    "        train(df): encoded train dataframe\n",
    "        test(df): encoded validation dataframe\n",
    "    '''\n",
    "    #label encoder\n",
    "    le = LabelEncoder()\n",
    "    #fit encoder to train dataset\n",
    "    trained_le = le.fit(train['class'])\n",
    "    #apply to all datasets\n",
    "    train['id'] = trained_le.transform(train['class'])\n",
    "    test['id'] = trained_le.transform(test['class'])\n",
    "    return train,test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encode and split for original vs augmented clean comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#original data\n",
    "#encoding\n",
    "train_enc, test_enc = encode(train_data,test_data) \n",
    "X_train, X_test = train_data['text'], test_data['text']\n",
    "y_train, y_test = pd.to_numeric(train_data['id']), pd.to_numeric(test_data['id'])\n",
    "\n",
    "# Separate the data for augmented cleaned data\n",
    "#encoding\n",
    "train_encA, test_encA = encode(train_caugl,test_cl)\n",
    "X_trainA, X_testA = train_encA['text'], test_encA['text']\n",
    "y_trainA, y_testA = pd.to_numeric(train_encA['id']), pd.to_numeric(test_encA['id'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Pipeline - Logistic Regression and Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create parameter grid for hyperparameter tuning via halving grid search\n",
    "#some solvers have specific penalties they can use\n",
    "param_grid = {\n",
    "    'Logistic Regression': [\n",
    "        {\n",
    "            'Logistic Regression__C': [0.01,0.1,1],\n",
    "            'Logistic Regression__penalty': ['l2'],\n",
    "            'Logistic Regression__solver': ['newton-cg','lbfgs'], #liblinear,sag,saga failed to converge\n",
    "            'Logistic Regression__max_iter': [50,100,200,300,500]\n",
    "        }\n",
    "    ],\n",
    "    'Random Forest': {\n",
    "        'Random Forest__n_estimators': [40,50,60,70,80],\n",
    "        'Random Forest__max_depth': [20,40,60,80]\n",
    "    }\n",
    "}\n",
    "\n",
    "#defining values for model arguments in pipelines\n",
    "##tfidf\n",
    "ngram_range = (1,2)\n",
    "min_df = 3\n",
    "max_features = 1000\n",
    "##models \n",
    "class_weight = 'balanced'\n",
    "oob_score=True\n",
    "##tuning\n",
    "cv = 5\n",
    "factor = 2\n",
    "resource = 'n_samples'\n",
    "verbose = 0\n",
    "scoring = 'neg_log_loss'\n",
    "\n",
    "#create pipelines which include vectorization for each model\n",
    "#data previously split using stratified k fold then it was also pre-processed, so I will just fit the model to the training data.\n",
    "pipelines_original_untuned = [ \n",
    "    ('Logistic Regression', Pipeline([\n",
    "        ('TF-IDF', TfidfVectorizer(ngram_range=ngram_range,min_df=min_df,max_features=max_features)),\n",
    "        ('Logistic Regression', LogisticRegression(class_weight=class_weight))\n",
    "    ])), \n",
    "    ('Random Forest', Pipeline([\n",
    "        ('TF-IDF', TfidfVectorizer(ngram_range=ngram_range,min_df=min_df,max_features=max_features)),\n",
    "        ('Random Forest', RandomForestClassifier(class_weight=class_weight,oob_score=oob_score))\n",
    "    ]))\n",
    "]\n",
    "\n",
    "pipelines_original_tuned = [\n",
    "    ('Logistic Regression', HalvingGridSearchCV(estimator=Pipeline([\n",
    "        ('TF-IDF', TfidfVectorizer(ngram_range=ngram_range,min_df=min_df,max_features=max_features)),\n",
    "        ('Logistic Regression', LogisticRegression(class_weight=class_weight))\n",
    "    ]), param_grid=param_grid['Logistic Regression'], cv=cv, factor=factor, resource=resource,scoring=scoring,verbose=verbose)), \n",
    "\n",
    "    ('Random Forest', HalvingGridSearchCV(estimator=Pipeline([\n",
    "        ('TF-IDF', TfidfVectorizer(ngram_range=ngram_range,min_df=min_df,max_features=max_features)),\n",
    "        ('Random Forest', RandomForestClassifier(class_weight=class_weight,oob_score=oob_score))\n",
    "    ]), param_grid=param_grid['Random Forest'], cv=cv, factor=factor, resource=resource,scoring=scoring,verbose=verbose))\n",
    "]\n",
    "\n",
    "pipelines_balanced_untuned = [\n",
    "    ('Logistic Regression', Pipeline([\n",
    "        ('TF-IDF', TfidfVectorizer(ngram_range=ngram_range,min_df=min_df,max_features=max_features)),\n",
    "        ('Logistic Regression', LogisticRegression())\n",
    "    ])), \n",
    "    ('Random Forest', Pipeline([\n",
    "        ('TF-IDF', TfidfVectorizer(ngram_range=ngram_range,min_df=min_df,max_features=max_features)),\n",
    "        ('Random Forest', RandomForestClassifier(oob_score=oob_score))\n",
    "    ]))\n",
    "]\n",
    "\n",
    "pipelines_balanced_tuned = [\n",
    "    ('Logistic Regression', HalvingGridSearchCV(estimator=Pipeline([\n",
    "        ('TF-IDF', TfidfVectorizer(ngram_range=ngram_range,min_df=min_df,max_features=max_features)),\n",
    "        ('Logistic Regression', LogisticRegression())\n",
    "    ]), param_grid=param_grid['Logistic Regression'], cv=cv, factor=factor, resource=resource,scoring=scoring,verbose=verbose)), \n",
    "\n",
    "    ('Random Forest', HalvingGridSearchCV(estimator=Pipeline([\n",
    "        ('TF-IDF', TfidfVectorizer(ngram_range=ngram_range,min_df=min_df,max_features=max_features)),\n",
    "        ('Random Forest', RandomForestClassifier(oob_score=oob_score))\n",
    "    ]), param_grid=param_grid['Random Forest'], cv=cv, factor=factor, resource=resource,scoring=scoring,verbose=verbose))\n",
    "]\n",
    "\n",
    "\n",
    "def train_evaluate(pipelines:list, X_train:pd.Series, y_train:pd.Series, X_test:pd.Series,y_test:pd.Series):\n",
    "    '''\n",
    "    this function runs through each model in the pipelines list, fits the model, makes predictions and \n",
    "    reports the performance metrics\n",
    "\n",
    "    Args:\n",
    "        X_train(pd.Series): train data\n",
    "        y_train(pd.Series): train labels\n",
    "        X_test(pd.Series): test data\n",
    "        y_test(pd.Series): test labels\n",
    "    Returns:\n",
    "        classification report(str): strings containing confusion matrix for each model in pipelines\n",
    "        results(dict): a dictionary containing f1,precision, recall,balanced accuracy scores, training and test errors for each model\n",
    "    '''\n",
    "    #initialise results dictionary\n",
    "    results = {}\n",
    "\n",
    "    for name, pipeline in pipelines:\n",
    "        #fit  model to training data\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        #print best parameters               \n",
    "        #print(f\"Best parameters for {name}: {pipeline.best_params_}\") #uncomment when tuning\n",
    "        #predict on test data\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "\n",
    "        #predict probabilities on train and test data\n",
    "        y_train_pred_prob = pipeline.predict_proba(X_train)\n",
    "        y_test_pred_prob = pipeline.predict_proba(X_test)\n",
    "\n",
    "        # Calculate log loss\n",
    "        train_error = log_loss(y_train, y_train_pred_prob)\n",
    "        test_error = log_loss(y_test, y_test_pred_prob)\n",
    "\n",
    "        #print classification report\n",
    "        print(f\"Classification report for {name}:\")\n",
    "        print(classification_report(y_test, \n",
    "                                    y_pred,\n",
    "                                    labels = [0,1],\n",
    "                                    target_names = ['negative','positive']))\n",
    "        \n",
    "        avg_f1 = f1_score(y_test, y_pred, average = 'micro') #micro, macro\n",
    "        avg_precision = precision_score(y_test, y_pred, average = 'micro')\n",
    "        avg_recall = recall_score(y_test, y_pred, average = 'micro')\n",
    "        b_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "\n",
    "        #save results for the model in dictionary\n",
    "        results[name] = {\n",
    "            'avg f1 score': avg_f1, \n",
    "            'avg precision score': avg_precision, \n",
    "            'avg recall score': avg_recall,\n",
    "            'balanced accuracy score': b_acc,\n",
    "            'train log loss': train_error,\n",
    "            'test log loss': test_error            \n",
    "        }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### performance on original data: tuned vs untuned hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance on Logistic Regression and Random Forest, using original data, untuned hyperparameters\n",
      "\n",
      "Classification report for Logistic Regression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.72      0.80      0.76       120\n",
      "    positive       0.91      0.86      0.89       273\n",
      "\n",
      "    accuracy                           0.84       393\n",
      "   macro avg       0.81      0.83      0.82       393\n",
      "weighted avg       0.85      0.84      0.85       393\n",
      "\n",
      "Classification report for Random Forest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.65      0.75       120\n",
      "    positive       0.86      0.97      0.91       273\n",
      "\n",
      "    accuracy                           0.87       393\n",
      "   macro avg       0.88      0.81      0.83       393\n",
      "weighted avg       0.87      0.87      0.86       393\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg f1 score</th>\n",
       "      <th>avg precision score</th>\n",
       "      <th>avg recall score</th>\n",
       "      <th>balanced accuracy score</th>\n",
       "      <th>train log loss</th>\n",
       "      <th>test log loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.844784</td>\n",
       "      <td>0.844784</td>\n",
       "      <td>0.844784</td>\n",
       "      <td>0.832234</td>\n",
       "      <td>0.378575</td>\n",
       "      <td>0.426973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.870229</td>\n",
       "      <td>0.870229</td>\n",
       "      <td>0.870229</td>\n",
       "      <td>0.808516</td>\n",
       "      <td>0.108822</td>\n",
       "      <td>0.362056</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     avg f1 score  avg precision score  avg recall score  \\\n",
       "Logistic Regression      0.844784             0.844784          0.844784   \n",
       "Random Forest            0.870229             0.870229          0.870229   \n",
       "\n",
       "                     balanced accuracy score  train log loss  test log loss  \n",
       "Logistic Regression                 0.832234        0.378575       0.426973  \n",
       "Random Forest                       0.808516        0.108822       0.362056  "
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Model Performance on Logistic Regression and Random Forest, using original data, untuned hyperparameters\\n')\n",
    "results_original_untuned = train_evaluate(pipelines_original_untuned, X_train, y_train, X_test,y_test)\n",
    "pd.DataFrame(results_original_untuned).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance on Logistic Regression and Random Forest, using original data, TUNED hyperparameters\n",
      "\n",
      "Best parameters for Logistic Regression: {'Logistic Regression__C': 1, 'Logistic Regression__max_iter': 200, 'Logistic Regression__penalty': 'l2', 'Logistic Regression__solver': 'newton-cg'}\n",
      "Classification report for Logistic Regression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.72      0.80      0.76       120\n",
      "    positive       0.91      0.86      0.89       273\n",
      "\n",
      "    accuracy                           0.84       393\n",
      "   macro avg       0.81      0.83      0.82       393\n",
      "weighted avg       0.85      0.84      0.85       393\n",
      "\n",
      "Best parameters for Random Forest: {'Random Forest__max_depth': 60, 'Random Forest__n_estimators': 50}\n",
      "Classification report for Random Forest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.68      0.77       120\n",
      "    positive       0.87      0.96      0.91       273\n",
      "\n",
      "    accuracy                           0.87       393\n",
      "   macro avg       0.87      0.82      0.84       393\n",
      "weighted avg       0.87      0.87      0.87       393\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg f1 score</th>\n",
       "      <th>avg precision score</th>\n",
       "      <th>avg recall score</th>\n",
       "      <th>balanced accuracy score</th>\n",
       "      <th>train log loss</th>\n",
       "      <th>test log loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.844784</td>\n",
       "      <td>0.844784</td>\n",
       "      <td>0.844784</td>\n",
       "      <td>0.832234</td>\n",
       "      <td>0.378579</td>\n",
       "      <td>0.426977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.872774</td>\n",
       "      <td>0.872774</td>\n",
       "      <td>0.872774</td>\n",
       "      <td>0.819689</td>\n",
       "      <td>0.137562</td>\n",
       "      <td>0.356193</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     avg f1 score  avg precision score  avg recall score  \\\n",
       "Logistic Regression      0.844784             0.844784          0.844784   \n",
       "Random Forest            0.872774             0.872774          0.872774   \n",
       "\n",
       "                     balanced accuracy score  train log loss  test log loss  \n",
       "Logistic Regression                 0.832234        0.378579       0.426977  \n",
       "Random Forest                       0.819689        0.137562       0.356193  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Model Performance on Logistic Regression and Random Forest, using original data, TUNED hyperparameters\\n')\n",
    "results_original_tuned = train_evaluate(pipelines_original_tuned, X_train, y_train, X_test,y_test)\n",
    "pd.DataFrame(results_original_tuned).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Performance on balanced data: tuned vs untuned hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance on Logistic Regression and Random Forest, using balanced clean data, untuned hyperparameters\n",
      "\n",
      "Classification report for Logistic Regression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.70      0.86      0.77       120\n",
      "    positive       0.93      0.84      0.88       273\n",
      "\n",
      "    accuracy                           0.84       393\n",
      "   macro avg       0.81      0.85      0.82       393\n",
      "weighted avg       0.86      0.84      0.85       393\n",
      "\n",
      "Classification report for Random Forest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.70      0.86      0.77       120\n",
      "    positive       0.93      0.84      0.88       273\n",
      "\n",
      "    accuracy                           0.84       393\n",
      "   macro avg       0.82      0.85      0.83       393\n",
      "weighted avg       0.86      0.84      0.85       393\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg f1 score</th>\n",
       "      <th>avg precision score</th>\n",
       "      <th>avg recall score</th>\n",
       "      <th>balanced accuracy score</th>\n",
       "      <th>train log loss</th>\n",
       "      <th>test log loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.824483</td>\n",
       "      <td>0.813279</td>\n",
       "      <td>0.846749</td>\n",
       "      <td>0.846749</td>\n",
       "      <td>0.374602</td>\n",
       "      <td>0.430126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.827001</td>\n",
       "      <td>0.815787</td>\n",
       "      <td>0.848581</td>\n",
       "      <td>0.848581</td>\n",
       "      <td>0.112919</td>\n",
       "      <td>0.391848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     avg f1 score  avg precision score  avg recall score  \\\n",
       "Logistic Regression      0.824483             0.813279          0.846749   \n",
       "Random Forest            0.827001             0.815787          0.848581   \n",
       "\n",
       "                     balanced accuracy score  train log loss  test log loss  \n",
       "Logistic Regression                 0.846749        0.374602       0.430126  \n",
       "Random Forest                       0.848581        0.112919       0.391848  "
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Model Performance on Logistic Regression and Random Forest, using balanced clean data, untuned hyperparameters\\n')\n",
    "results_balanced = train_evaluate(pipelines_balanced_untuned, X_trainA, y_trainA, X_testA,y_testA)\n",
    "pd.DataFrame(results_balanced).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance on Logistic Regression and Random Forest, using balanced clean data, TUNED hyperparameters\n",
      "\n",
      "Best parameters for Logistic Regression: {'Logistic Regression__C': 1, 'Logistic Regression__max_iter': 500, 'Logistic Regression__penalty': 'l2', 'Logistic Regression__solver': 'newton-cg'}\n",
      "Classification report for Logistic Regression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.70      0.86      0.77       120\n",
      "    positive       0.93      0.84      0.88       273\n",
      "\n",
      "    accuracy                           0.84       393\n",
      "   macro avg       0.81      0.85      0.82       393\n",
      "weighted avg       0.86      0.84      0.85       393\n",
      "\n",
      "Best parameters for Random Forest: {'Random Forest__max_depth': 40, 'Random Forest__n_estimators': 80}\n",
      "Classification report for Random Forest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.86      0.77       120\n",
      "    positive       0.93      0.83      0.88       273\n",
      "\n",
      "    accuracy                           0.84       393\n",
      "   macro avg       0.81      0.84      0.82       393\n",
      "weighted avg       0.86      0.84      0.84       393\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg f1 score</th>\n",
       "      <th>avg precision score</th>\n",
       "      <th>avg recall score</th>\n",
       "      <th>balanced accuracy score</th>\n",
       "      <th>train log loss</th>\n",
       "      <th>test log loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.824483</td>\n",
       "      <td>0.813279</td>\n",
       "      <td>0.846749</td>\n",
       "      <td>0.846749</td>\n",
       "      <td>0.374602</td>\n",
       "      <td>0.430126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.821971</td>\n",
       "      <td>0.810802</td>\n",
       "      <td>0.844918</td>\n",
       "      <td>0.844918</td>\n",
       "      <td>0.310247</td>\n",
       "      <td>0.435566</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     avg f1 score  avg precision score  avg recall score  \\\n",
       "Logistic Regression      0.824483             0.813279          0.846749   \n",
       "Random Forest            0.821971             0.810802          0.844918   \n",
       "\n",
       "                     balanced accuracy score  train log loss  test log loss  \n",
       "Logistic Regression                 0.846749        0.374602       0.430126  \n",
       "Random Forest                       0.844918        0.310247       0.435566  "
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Model Performance on Logistic Regression and Random Forest, using balanced clean data, TUNED hyperparameters\\n')\n",
    "results_balanced_tuned = train_evaluate(pipelines_balanced_tuned, X_trainA, y_trainA, X_testA,y_testA)\n",
    "pd.DataFrame(results_balanced_tuned).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Pipeline distilRoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/alol/.cache/huggingface/hub/models--distilroberta-base/snapshots/d5411c3ee9e1793fd9ef58390b40a80a4c10df32/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"distilroberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /Users/alol/.cache/huggingface/hub/models--distilroberta-base/snapshots/d5411c3ee9e1793fd9ef58390b40a80a4c10df32/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "#modeled from well-known bert\n",
    "#distilroberta, twice as fast as roberta\n",
    "\n",
    "#from huggingface: https://huggingface.co/transformers/v3.4.0/custom_datasets.html \n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    ''' \n",
    "    This class uses the Dataset class from PyTorch. It turns labels and tokenized data into a torch dataset object for model ingestion.\n",
    "        \n",
    "    '''\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "\n",
    "def tokenize_traintest(X_train:pd.Series, y_train:pd.Series, X_test:pd.Series,y_test:pd.Series, tokenizer):\n",
    "    ''' \n",
    "    This function takes 4 series and a tokenizer class. It converts the series into lists, tokenises them and creates torch datasets using the Dataset class.\n",
    "\n",
    "    Args:\n",
    "        X_train(pd.Series): train text data\n",
    "        y_train(pd.Series): train label data\n",
    "        X_test(pd.Series): test text data\n",
    "        y_test(pd.Series): test label data\n",
    "        tokenizer(class): transformers RobertaTokenizerFast class\n",
    "\n",
    "    Returns:\n",
    "        train_dataset(torch df): torch train dataset\n",
    "        test_dataset(torch df): torch test dataset\n",
    "    '''\n",
    "    X_train, X_test = X_train.tolist(),X_test.tolist()\n",
    "    y_train, y_test = y_train.tolist(), y_test.tolist()\n",
    "\n",
    "    X_train_tokenized = tokenizer(X_train, truncation=True, padding=True, max_length=512)\n",
    "    X_test_tokenized = tokenizer(X_test, truncation=True, padding=True, max_length=512)\n",
    "\n",
    "    train_dataset = Dataset(X_train_tokenized, y_train)\n",
    "    test_dataset = Dataset(X_test_tokenized, y_test)\n",
    "\n",
    "    return train_dataset,test_dataset\n",
    "\n",
    "\n",
    "\n",
    "def compute_metrics(p): \n",
    "    \"\"\"\n",
    "    takes predictions outputted from hyperparameter tuning or fine-tuning, makes sure the predictions and label lengths are the same, \n",
    "    prints classification report from sklearn, print avg f1, avg precision, avg recall and balanced accuracy\n",
    "\n",
    "    Args:\n",
    "        p:   EvalPrediction object which are the predictions from the model\n",
    "\n",
    "    Returns:\n",
    "        avg f1, avg macro, avg precision, avg recall, balanced accuracy, classification report(str): strings containing performance metrics\n",
    "    \"\"\"\n",
    "    y_pred = np.argmax(p.predictions, axis = -1)\n",
    "    assert len(y_pred) == len(p.label_ids)\n",
    "    print(classification_report(p.label_ids, \n",
    "                                y_pred,\n",
    "                                labels = [0,1],\n",
    "                                target_names = ['negative','positive']))\n",
    "\n",
    "    avg_f1 = f1_score(p.label_ids, y_pred,average = 'macro') #micro, macro)\n",
    "    avg_precision = precision_score(p.label_ids, y_pred,average = 'macro') #micro, macro)\n",
    "    avg_recall = recall_score(p.label_ids, y_pred,average = 'macro') #micro, macro)\n",
    "    b_acc = balanced_accuracy_score(p.label_ids, y_pred)\n",
    "    return { \n",
    "            'macro_f1' : avg_f1, \n",
    "            'macro_precision': avg_precision, \n",
    "            'macro_recall': avg_recall,\n",
    "            'macro_balanced_accuracy': b_acc\n",
    "            }\n",
    "\n",
    "#initialise tokenizer\n",
    "robtokenizer = RobertaTokenizerFast.from_pretrained('distilroberta-base',no_deprecation_warning=True)\n",
    "\n",
    "#create tokenized datasets\n",
    "train_dataset,test_dataset = tokenize_traintest(X_train, y_train, X_test,y_test,robtokenizer) #original data\n",
    "train_datasetA,test_datasetA = tokenize_traintest(X_trainA, y_trainA, X_testA,y_testA,robtokenizer) #balanced clean data\n",
    "\n",
    "#initialise pre-trained distilroberta\n",
    "model = RobertaForSequenceClassification.from_pretrained('distilroberta-base', num_labels=2)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "        output_dir='./distilroberta_results',\n",
    "        num_train_epochs=6, #epoch5  underfitting\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=32,\n",
    "        weight_decay=0.01, #recommended\n",
    "        learning_rate= 0.00005, #default\n",
    "        metric_for_best_model = 'eval_loss',\n",
    "        logging_strategy = 'steps',\n",
    "        evaluation_strategy = 'steps',\n",
    "        save_strategy = 'steps', \n",
    "        eval_steps = 34.5, # for original data used 25 (every 1/2 epoch for both datasets)\n",
    "        save_steps = 34.5,\n",
    "        save_total_limit = 2,\n",
    "        load_best_model_at_end = True,\n",
    "        disable_tqdm = True\n",
    "    )\n",
    "\n",
    "trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_datasetA,\n",
    "        eval_dataset=test_datasetA,\n",
    "        compute_metrics = compute_metrics,\n",
    "        callbacks = [EarlyStoppingCallback(early_stopping_patience = 4)] # for original data used 2 (every 2 epoch)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and evaluate on original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alol/Documents/Documents/Python/Finan_BClass/fin-class/lib/python3.11/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1,574\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 300\n",
      "  Number of trainable parameters = 82,119,938\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 393\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./distilroberta_results/checkpoint-25\n",
      "Configuration saved in ./distilroberta_results/checkpoint-25/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.95      0.89       120\n",
      "    positive       0.98      0.92      0.95       273\n",
      "\n",
      "    accuracy                           0.93       393\n",
      "   macro avg       0.91      0.94      0.92       393\n",
      "weighted avg       0.94      0.93      0.93       393\n",
      "\n",
      "{'eval_loss': 0.2052389681339264, 'eval_micro_f1': 0.931297709923664, 'eval_micro_precision': 0.9312977099236641, 'eval_micro_recall': 0.9312977099236641, 'eval_micro_balanced_accuracy': 0.9365384615384615, 'eval_runtime': 4.1009, 'eval_samples_per_second': 95.833, 'eval_steps_per_second': 3.17, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [distilroberta_results/checkpoint-75] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 393\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./distilroberta_results/checkpoint-50\n",
      "Configuration saved in ./distilroberta_results/checkpoint-50/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.94      0.91      0.92       120\n",
      "    positive       0.96      0.97      0.97       273\n",
      "\n",
      "    accuracy                           0.95       393\n",
      "   macro avg       0.95      0.94      0.95       393\n",
      "weighted avg       0.95      0.95      0.95       393\n",
      "\n",
      "{'eval_loss': 0.14473921060562134, 'eval_micro_f1': 0.9541984732824428, 'eval_micro_precision': 0.9541984732824428, 'eval_micro_recall': 0.9541984732824428, 'eval_micro_balanced_accuracy': 0.9413461538461538, 'eval_runtime': 4.0997, 'eval_samples_per_second': 95.861, 'eval_steps_per_second': 3.171, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [distilroberta_results/checkpoint-150] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 393\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./distilroberta_results/checkpoint-75\n",
      "Configuration saved in ./distilroberta_results/checkpoint-75/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.96      0.94       120\n",
      "    positive       0.98      0.96      0.97       273\n",
      "\n",
      "    accuracy                           0.96       393\n",
      "   macro avg       0.95      0.96      0.96       393\n",
      "weighted avg       0.96      0.96      0.96       393\n",
      "\n",
      "{'eval_loss': 0.11818276345729828, 'eval_micro_f1': 0.9618320610687023, 'eval_micro_precision': 0.9618320610687023, 'eval_micro_recall': 0.9618320610687023, 'eval_micro_balanced_accuracy': 0.9608516483516484, 'eval_runtime': 4.1732, 'eval_samples_per_second': 94.172, 'eval_steps_per_second': 3.115, 'epoch': 1.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [distilroberta_results/checkpoint-25] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 393\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./distilroberta_results/checkpoint-100\n",
      "Configuration saved in ./distilroberta_results/checkpoint-100/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.94      0.97      0.95       120\n",
      "    positive       0.99      0.97      0.98       273\n",
      "\n",
      "    accuracy                           0.97       393\n",
      "   macro avg       0.96      0.97      0.97       393\n",
      "weighted avg       0.97      0.97      0.97       393\n",
      "\n",
      "{'eval_loss': 0.13203909993171692, 'eval_micro_f1': 0.9720101781170484, 'eval_micro_precision': 0.9720101781170484, 'eval_micro_recall': 0.9720101781170484, 'eval_micro_balanced_accuracy': 0.9705128205128205, 'eval_runtime': 4.1131, 'eval_samples_per_second': 95.549, 'eval_steps_per_second': 3.161, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [distilroberta_results/checkpoint-50] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 393\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./distilroberta_results/checkpoint-125\n",
      "Configuration saved in ./distilroberta_results/checkpoint-125/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.97      0.90      0.94       120\n",
      "    positive       0.96      0.99      0.97       273\n",
      "\n",
      "    accuracy                           0.96       393\n",
      "   macro avg       0.97      0.94      0.95       393\n",
      "weighted avg       0.96      0.96      0.96       393\n",
      "\n",
      "{'eval_loss': 0.18768490850925446, 'eval_micro_f1': 0.9618320610687023, 'eval_micro_precision': 0.9618320610687023, 'eval_micro_recall': 0.9618320610687023, 'eval_micro_balanced_accuracy': 0.9445054945054945, 'eval_runtime': 4.1369, 'eval_samples_per_second': 95.0, 'eval_steps_per_second': 3.142, 'epoch': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [distilroberta_results/checkpoint-100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 393\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./distilroberta_results/checkpoint-150\n",
      "Configuration saved in ./distilroberta_results/checkpoint-150/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.96      0.91      0.94       120\n",
      "    positive       0.96      0.99      0.97       273\n",
      "\n",
      "    accuracy                           0.96       393\n",
      "   macro avg       0.96      0.95      0.95       393\n",
      "weighted avg       0.96      0.96      0.96       393\n",
      "\n",
      "{'eval_loss': 0.14021770656108856, 'eval_micro_f1': 0.9618320610687023, 'eval_micro_precision': 0.9618320610687023, 'eval_micro_recall': 0.9618320610687023, 'eval_micro_balanced_accuracy': 0.9468406593406593, 'eval_runtime': 4.0496, 'eval_samples_per_second': 97.046, 'eval_steps_per_second': 3.21, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [distilroberta_results/checkpoint-125] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 393\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./distilroberta_results/checkpoint-175\n",
      "Configuration saved in ./distilroberta_results/checkpoint-175/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.97      0.90      0.94       120\n",
      "    positive       0.96      0.99      0.97       273\n",
      "\n",
      "    accuracy                           0.96       393\n",
      "   macro avg       0.97      0.94      0.95       393\n",
      "weighted avg       0.96      0.96      0.96       393\n",
      "\n",
      "{'eval_loss': 0.18575119972229004, 'eval_micro_f1': 0.9618320610687023, 'eval_micro_precision': 0.9618320610687023, 'eval_micro_recall': 0.9618320610687023, 'eval_micro_balanced_accuracy': 0.9445054945054945, 'eval_runtime': 4.1255, 'eval_samples_per_second': 95.26, 'eval_steps_per_second': 3.151, 'epoch': 3.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [distilroberta_results/checkpoint-150] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./distilroberta_results/checkpoint-75 (score: 0.11818276345729828).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 393\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 244.314, 'train_samples_per_second': 38.655, 'train_steps_per_second': 1.228, 'train_loss': 0.16602650233677455, 'epoch': 3.5}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.96      0.94       120\n",
      "    positive       0.98      0.96      0.97       273\n",
      "\n",
      "    accuracy                           0.96       393\n",
      "   macro avg       0.95      0.96      0.96       393\n",
      "weighted avg       0.96      0.96      0.96       393\n",
      "\n",
      "{'eval_loss': 0.11818276345729828, 'eval_micro_f1': 0.9618320610687023, 'eval_micro_precision': 0.9618320610687023, 'eval_micro_recall': 0.9618320610687023, 'eval_micro_balanced_accuracy': 0.9608516483516484, 'eval_runtime': 4.2669, 'eval_samples_per_second': 92.105, 'eval_steps_per_second': 3.047, 'epoch': 3.5}\n",
      "{'eval_loss': 0.11818276345729828, 'eval_micro_f1': 0.9618320610687023, 'eval_micro_precision': 0.9618320610687023, 'eval_micro_recall': 0.9618320610687023, 'eval_micro_balanced_accuracy': 0.9608516483516484, 'eval_runtime': 4.2669, 'eval_samples_per_second': 92.105, 'eval_steps_per_second': 3.047, 'epoch': 3.5}\n"
     ]
    }
   ],
   "source": [
    "#Train and evaluate on original data\n",
    "trainer.train()\n",
    "eval_metrics = trainer.evaluate()\n",
    "print(eval_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and evaluate on balanced clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alol/Documents/Documents/Python/Finan_BClass/fin-class/lib/python3.11/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2,180\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 414\n",
      "  Number of trainable parameters = 82,119,938\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 393\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./distilroberta_results/checkpoint-69\n",
      "Configuration saved in ./distilroberta_results/checkpoint-69/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.87      0.85       120\n",
      "    positive       0.94      0.92      0.93       273\n",
      "\n",
      "    accuracy                           0.90       393\n",
      "   macro avg       0.88      0.89      0.89       393\n",
      "weighted avg       0.91      0.90      0.90       393\n",
      "\n",
      "{'eval_loss': 0.2972542941570282, 'eval_macro_f1': 0.8875790424570912, 'eval_macro_precision': 0.8827358658819333, 'eval_macro_recall': 0.893040293040293, 'eval_macro_balanced_accuracy': 0.893040293040293, 'eval_runtime': 3.5994, 'eval_samples_per_second': 109.184, 'eval_steps_per_second': 3.612, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [distilroberta_results/checkpoint-207] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 393\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./distilroberta_results/checkpoint-138\n",
      "Configuration saved in ./distilroberta_results/checkpoint-138/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.65      0.97      0.78       120\n",
      "    positive       0.98      0.77      0.86       273\n",
      "\n",
      "    accuracy                           0.83       393\n",
      "   macro avg       0.82      0.87      0.82       393\n",
      "weighted avg       0.88      0.83      0.84       393\n",
      "\n",
      "{'eval_loss': 0.38455986976623535, 'eval_macro_f1': 0.8216387941467709, 'eval_macro_precision': 0.8165403710478181, 'eval_macro_recall': 0.8697802197802198, 'eval_macro_balanced_accuracy': 0.8697802197802198, 'eval_runtime': 3.3741, 'eval_samples_per_second': 116.475, 'eval_steps_per_second': 3.853, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [distilroberta_results/checkpoint-414] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 393\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./distilroberta_results/checkpoint-207\n",
      "Configuration saved in ./distilroberta_results/checkpoint-207/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.92      0.88       120\n",
      "    positive       0.96      0.93      0.94       273\n",
      "\n",
      "    accuracy                           0.92       393\n",
      "   macro avg       0.90      0.92      0.91       393\n",
      "weighted avg       0.93      0.92      0.92       393\n",
      "\n",
      "{'eval_loss': 0.2319410890340805, 'eval_macro_f1': 0.9120149253731342, 'eval_macro_precision': 0.9040655162328166, 'eval_macro_recall': 0.9217032967032968, 'eval_macro_balanced_accuracy': 0.9217032967032968, 'eval_runtime': 3.5019, 'eval_samples_per_second': 112.225, 'eval_steps_per_second': 3.712, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [distilroberta_results/checkpoint-69] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 393\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./distilroberta_results/checkpoint-276\n",
      "Configuration saved in ./distilroberta_results/checkpoint-276/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.87      0.89       120\n",
      "    positive       0.94      0.97      0.95       273\n",
      "\n",
      "    accuracy                           0.94       393\n",
      "   macro avg       0.93      0.92      0.92       393\n",
      "weighted avg       0.94      0.94      0.94       393\n",
      "\n",
      "{'eval_loss': 0.22930088639259338, 'eval_macro_f1': 0.9237479530302912, 'eval_macro_precision': 0.9316055625790138, 'eval_macro_recall': 0.9168498168498169, 'eval_macro_balanced_accuracy': 0.9168498168498169, 'eval_runtime': 3.3118, 'eval_samples_per_second': 118.667, 'eval_steps_per_second': 3.925, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [distilroberta_results/checkpoint-138] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 393\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./distilroberta_results/checkpoint-345\n",
      "Configuration saved in ./distilroberta_results/checkpoint-345/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.93      0.86       120\n",
      "    positive       0.96      0.90      0.93       273\n",
      "\n",
      "    accuracy                           0.91       393\n",
      "   macro avg       0.89      0.91      0.90       393\n",
      "weighted avg       0.92      0.91      0.91       393\n",
      "\n",
      "{'eval_loss': 0.3726854920387268, 'eval_macro_f1': 0.8988253293417579, 'eval_macro_precision': 0.8875313640510949, 'eval_macro_recall': 0.9148809523809525, 'eval_macro_balanced_accuracy': 0.9148809523809525, 'eval_runtime': 3.4601, 'eval_samples_per_second': 113.581, 'eval_steps_per_second': 3.757, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [distilroberta_results/checkpoint-207] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 393\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./distilroberta_results/checkpoint-414\n",
      "Configuration saved in ./distilroberta_results/checkpoint-414/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.91      0.86       120\n",
      "    positive       0.96      0.91      0.93       273\n",
      "\n",
      "    accuracy                           0.91       393\n",
      "   macro avg       0.89      0.91      0.90       393\n",
      "weighted avg       0.92      0.91      0.91       393\n",
      "\n",
      "{'eval_loss': 0.33656203746795654, 'eval_macro_f1': 0.897997018887793, 'eval_macro_precision': 0.8886205899363795, 'eval_macro_recall': 0.9102106227106227, 'eval_macro_balanced_accuracy': 0.9102106227106227, 'eval_runtime': 3.5263, 'eval_samples_per_second': 111.448, 'eval_steps_per_second': 3.687, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [distilroberta_results/checkpoint-345] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./distilroberta_results/checkpoint-276 (score: 0.22930088639259338).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 393\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 433.1966, 'train_samples_per_second': 30.194, 'train_steps_per_second': 0.956, 'train_loss': 0.17743848717730978, 'epoch': 6.0}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.87      0.89       120\n",
      "    positive       0.94      0.97      0.95       273\n",
      "\n",
      "    accuracy                           0.94       393\n",
      "   macro avg       0.93      0.92      0.92       393\n",
      "weighted avg       0.94      0.94      0.94       393\n",
      "\n",
      "{'eval_loss': 0.22930088639259338, 'eval_macro_f1': 0.9237479530302912, 'eval_macro_precision': 0.9316055625790138, 'eval_macro_recall': 0.9168498168498169, 'eval_macro_balanced_accuracy': 0.9168498168498169, 'eval_runtime': 3.4485, 'eval_samples_per_second': 113.964, 'eval_steps_per_second': 3.77, 'epoch': 6.0}\n",
      "{'eval_loss': 0.22930088639259338, 'eval_macro_f1': 0.9237479530302912, 'eval_macro_precision': 0.9316055625790138, 'eval_macro_recall': 0.9168498168498169, 'eval_macro_balanced_accuracy': 0.9168498168498169, 'eval_runtime': 3.4485, 'eval_samples_per_second': 113.964, 'eval_steps_per_second': 3.77, 'epoch': 6.0}\n"
     ]
    }
   ],
   "source": [
    "#Train and evaluate on balanced clean data\n",
    "trainer.train()\n",
    "eval_metrics = trainer.evaluate()\n",
    "print(eval_metrics)"
   ]
  },
  
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8a8a0c247a878aefa27de50ed94ed7bf72786d20260a627287ec2a7df6d66286"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
